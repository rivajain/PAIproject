{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0063de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0313bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d213ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f2f8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a07fd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d905db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81eec261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cb050e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d5b8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2906d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e7a96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10875c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb21ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb18b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0899449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a760a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8754\n",
      "Epoch 1: loss improved from inf to 7.87535, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 204ms/step - loss: 7.8754 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8616\n",
      "Epoch 2: loss improved from 7.87535 to 7.86160, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 223ms/step - loss: 7.8616 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8117\n",
      "Epoch 3: loss improved from 7.86160 to 7.81170, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 227ms/step - loss: 7.8117 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.6139\n",
      "Epoch 4: loss improved from 7.81170 to 7.61392, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 214ms/step - loss: 7.6139 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.4149\n",
      "Epoch 5: loss improved from 7.61392 to 7.41490, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 7.4149 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.2553\n",
      "Epoch 6: loss improved from 7.41490 to 7.25532, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 247ms/step - loss: 7.2553 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.1468\n",
      "Epoch 7: loss improved from 7.25532 to 7.14677, saving model to nextword1.h5\n",
      "61/61 [==============================] - 15s 240ms/step - loss: 7.1468 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0540\n",
      "Epoch 8: loss improved from 7.14677 to 7.05396, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 228ms/step - loss: 7.0540 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.9141\n",
      "Epoch 9: loss improved from 7.05396 to 6.91408, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 222ms/step - loss: 6.9141 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.7102\n",
      "Epoch 10: loss improved from 6.91408 to 6.71021, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 220ms/step - loss: 6.7102 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4724\n",
      "Epoch 11: loss improved from 6.71021 to 6.47239, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 223ms/step - loss: 6.4724 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.2264\n",
      "Epoch 12: loss improved from 6.47239 to 6.22642, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 211ms/step - loss: 6.2264 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.0036\n",
      "Epoch 13: loss improved from 6.22642 to 6.00358, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 208ms/step - loss: 6.0036 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.8061\n",
      "Epoch 14: loss improved from 6.00358 to 5.80613, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 206ms/step - loss: 5.8061 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.6124\n",
      "Epoch 15: loss improved from 5.80613 to 5.61239, saving model to nextword1.h5\n",
      "61/61 [==============================] - 12s 194ms/step - loss: 5.6124 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.4537\n",
      "Epoch 16: loss improved from 5.61239 to 5.45369, saving model to nextword1.h5\n",
      "61/61 [==============================] - 12s 194ms/step - loss: 5.4537 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.3199\n",
      "Epoch 17: loss improved from 5.45369 to 5.31994, saving model to nextword1.h5\n",
      "61/61 [==============================] - 13s 207ms/step - loss: 5.3199 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.1712\n",
      "Epoch 18: loss improved from 5.31994 to 5.17117, saving model to nextword1.h5\n",
      "61/61 [==============================] - 12s 202ms/step - loss: 5.1712 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.0357\n",
      "Epoch 19: loss improved from 5.17117 to 5.03572, saving model to nextword1.h5\n",
      "61/61 [==============================] - 14s 229ms/step - loss: 5.0357 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.9473\n",
      "Epoch 20: loss improved from 5.03572 to 4.94733, saving model to nextword1.h5\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7299a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n",
    "# tensorboard --logdir=\"./logsnextword1\"\n",
    "# http://DESKTOP-U3TSCVT:6006/\n",
    "\n",
    "from IPython.display import Image \n",
    "pil_img = Image(filename='graph1.png')\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04203c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
